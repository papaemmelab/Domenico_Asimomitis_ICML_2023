{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61cbe47-386a-4d71-9a88-c600deea82c1",
   "metadata": {},
   "source": [
    "# High-Resolution Interpretable Classification of Artifacts versus Real Variants in Whole Genome Sequencing Data from Archived Tissue  <br/> Domenico & Asimomitis et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335eaac-fbba-4493-ae14-4b4668e1d240",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a8f9d-72f6-42dc-93ed-e6d7a8ada910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from os.path import join, exists\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import functools\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from Model_class import *\n",
    "from ICML_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd0600-c1ad-4859-a65d-4e429c85f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "LOG_DIR = \"../logs\"\n",
    "if not exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bb8ea-a7a3-4b81-9b7c-7f04b20c867a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76584115-bdc1-4eef-b675-52570e9ae569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_three_channels(t, c1=0, c2=2, c3=5):\n",
    "    t = t.numpy()\n",
    "    t = cv2.merge([t[:,:,c1], t[:,:,c2], t[:,:,c3]])\n",
    "    return tf.convert_to_tensor(t)\n",
    "\n",
    "def _parse_function(proto, c1, c2, c3):\n",
    "    features = {\n",
    "        \"image/encoded\": tf.io.FixedLenFeature((), tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature((), tf.int64),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(proto, features)\n",
    "\n",
    "    x = tf.reshape(tf.io.decode_raw(parsed_features['image/encoded'], tf.uint8), [100, 221, 6])\n",
    "    \n",
    "    # extra code to extract 3 specific channels - comment out or in to use or not use\n",
    "    x = tf.py_function(func=convert_to_three_channels, inp=[x, c1, c2, c3], Tout=tf.uint8)\n",
    "    # extract label\n",
    "    y = tf.cast(parsed_features['label'], tf.int64)\n",
    "    return x, y\n",
    "\n",
    "def load_dataset(data, batch_size, c1=0, c2=2, c3=5):\n",
    "    new_parse_function = functools.partial(_parse_function, c1=c1, c2=c2, c3=c3)\n",
    "    dataset = data.map(new_parse_function, num_parallel_calls=16)\n",
    "    dataset = dataset.batch(batch_size)#.prefetch(1)  # batch and prefetch\n",
    "\n",
    "    return iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b9f68-e31b-4846-8c77-ca7fc99e7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_to_torch(tensor):\n",
    "    batch_size = tensor.shape[0]\n",
    "    output_torch = np.zeros(shape=(batch_size, 3, 100, 221)) # 100x221 to 221x221\n",
    "    for idx, im in enumerate(tensor):\n",
    "        image = np.transpose(im.numpy(),(2,0,1))\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        output_torch[idx] = image\n",
    "    output_torch = torch.from_numpy(output_torch)\n",
    "    output_torch = output_torch.type(torch.DoubleTensor)\n",
    "    return output_torch.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ada7e0-309a-4b75-848b-a4830f51460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(content, filename):\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b646e9-9503-4c61-b205-140e0499ad49",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce63b23-1782-4ba2-89c9-1ef925af030f",
   "metadata": {},
   "source": [
    "The first step is passing the variant through DeepVariant's <b>make_examples</b> module. The details for this are located here: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#make_examples\n",
    "\n",
    "Mutation pileup images are stored in TFRecord format protos and can be manipulated further using Tensorflow. The user can generate pileup images for any variant call set of choice using the <i>--variant_caller vcf_candidate_importer</i> option, they must provide <b>make_examples</b> with a VCF (<i>--proposed_variants</i>), variant BED (<i>--regions</i>), BAM (<i>--reads</i>), and reference FASTA (<i>--ref</i>).\n",
    "\n",
    "Once generated, the user can read in these files as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970ac0e-79e1-4c5a-86fd-6b9535ae7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_dataset = tf.data.TFRecordDataset(join(DATA_DIR, \"artifact.tfrecord.gz\"), compression_type=\"GZIP\")\n",
    "real_variant_dataset = tf.data.TFRecordDataset(join(DATA_DIR, \"real_variant.tfrecord.gz\"), compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151d1ab-fba9-4708-951d-a58fe095dc26",
   "metadata": {},
   "source": [
    "then convert to PNGs (for ease of viewing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b31ca5-92f3-4c76-9edc-ba7dc13032eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_iterator = load_dataset(artifact_dataset, 1)\n",
    "for _, (data, _) in enumerate(artifact_iterator):\n",
    "    data = tf_to_torch(data)\n",
    "    for idx, im in enumerate(data):\n",
    "        save_tensor_as_png(im, join(DATA_DIR, \"artifact.png\"))\n",
    "real_iterator = load_dataset(real_variant_dataset, 1)\n",
    "for _, (data, _) in enumerate(real_iterator):\n",
    "    data = tf_to_torch(data)\n",
    "    for idx, im in enumerate(data):\n",
    "        save_tensor_as_png(im, join(DATA_DIR, \"real_variant.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c729cec-7f5b-49fa-9d5c-a483c3d3c178",
   "metadata": {},
   "source": [
    "# 2. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f9271-c7a1-4e9a-ae45-74dbfaa766f3",
   "metadata": {},
   "source": [
    "For training the following needs to be specified below:\n",
    "- Training/Validation directories should be specified below and filled with example images.\n",
    "- A dataframe labeling pileup images as real or artifact with a column for filename (excluding extension) and a column for the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f071e0c-bfad-4e85-b8f3-c0f0bbd1ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/work/isabl/home/domenicd/benchmarking/ffpe/cnn/notebook/icml_images/train\"\n",
    "valid_dir = \"/work/isabl/home/domenicd/benchmarking/ffpe/cnn/notebook/icml_images/valid\"\n",
    "labels_path = join(\"/work/isabl/home/domenicd/benchmarking/ffpe/cnn/notebook/icml_images\", \"labels.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce701f0-8312-4a61-b910-47c5213cef4c",
   "metadata": {},
   "source": [
    "Hyperparameters can be adjusted and defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298644a-1b06-470a-b0b8-ae1c14fdece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 10\n",
    "num_classes = 2\n",
    "lr = 0.001\n",
    "BATCH_SIZE = 16\n",
    "model_name = \"test\"\n",
    "torch.set_num_threads(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a40241-ba6a-4f92-8b9a-7b79724996cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = MyModel(pretrained=False, n_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.Model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09dbbd-92db-4c2c-aafb-205dbe3b9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "runstart=datetime.datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "start = time.time()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "labels = pd.read_csv(labels_path, sep=\"\\t\").set_index(\"name\")\n",
    "train_data = glob.glob(os.path.join(train_dir, \"*.png\"))\n",
    "valid_data = glob.glob(os.path.join(valid_dir, \"*.png\"))\n",
    "\n",
    "TRAIN_SIZE = len(train_data)\n",
    "VAL_SIZE = len(valid_data)\n",
    "\n",
    "train_dataset = [(x, labels.loc[x.split(\"/\")[-1].strip(\".png\"), \"label\"]) for x in train_data]\n",
    "valid_dataset = [(x, labels.loc[x.split(\"/\")[-1].strip(\".png\"), \"label\"]) for x in valid_data]\n",
    "\n",
    "# Run training\n",
    "logfile = join(LOG_DIR, f\"{runstart}_training.log\")\n",
    "print(logfile)\n",
    "print(\"Executing training\")\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    random.shuffle(train_dataset)\n",
    "    train_batches = [train_dataset[i:i + BATCH_SIZE] for i in range(0, len(train_dataset), BATCH_SIZE)]\n",
    "\n",
    "    model.train()\n",
    "    print(\"\")\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for batch_i, (dataset) in enumerate(train_batches):\n",
    "        print(f\"Batch {batch_i+1}/{round(TRAIN_SIZE/BATCH_SIZE)}\", end='\\r')\n",
    "        images, targets = [x[0] for x in dataset],[x[1] for x in dataset]\n",
    "        data = torch.Tensor(BATCH_SIZE, 3, 221, 221)\n",
    "        for idx, im in enumerate(images):\n",
    "            _, images[idx] = read_png_as_tensor(im, 221, 221)\n",
    "        torch.cat(images, out=data)\n",
    "        data = data.type(torch.DoubleTensor).float()\n",
    "        target = torch.from_numpy(np.array(targets)).unsqueeze(-1).long()\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model.forward(data)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, target.squeeze(dim=1))\n",
    "\n",
    "        # Backward and optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = probabilities.argmax(dim=1)\n",
    "        correct += sum([val==target.numpy()[:,0][idx] for idx,val in enumerate(predicted)])\n",
    "        total += len(predicted)\n",
    "        if (batch_i+1) % 1 == 0:\n",
    "            accuracy = 100 * correct / total\n",
    "            write_log('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Training Accuracy: {:.2f}%\\n'\n",
    "                 .format(epoch+1, num_epochs, batch_i+1, round(TRAIN_SIZE/BATCH_SIZE), loss.item(), accuracy), logfile)\n",
    "            end = time.time()\n",
    "            write_log(f\"Time elapsed: {end-start:.02f} seconds\\n\", logfile)\n",
    "            break\n",
    "\n",
    "    # check validation accuracy\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    random.shuffle(valid_dataset)\n",
    "    valid_batches = [valid_dataset[i:i + BATCH_SIZE] for i in range(0, len(valid_dataset), BATCH_SIZE)]\n",
    "    for batch_i, (dataset) in enumerate(valid_batches):\n",
    "        images, targets = [x[0] for x in dataset],[x[1] for x in dataset]\n",
    "        data = torch.Tensor(BATCH_SIZE, 3, 221, 221)\n",
    "        for idx, im in enumerate(images):\n",
    "            _, images[idx] = read_png_as_tensor(im, 221, 221)\n",
    "        torch.cat(images, out=data)\n",
    "        data = data.type(torch.DoubleTensor).float()\n",
    "        target = torch.from_numpy(np.array(targets)).unsqueeze(-1).long()\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model.forward(data)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, target.squeeze(dim=1))\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted = probabilities.argmax(dim=1)\n",
    "        correct += sum([val==target.numpy()[:,0][idx] for idx,val in enumerate(predicted)])\n",
    "        total += len(predicted)\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_loss = running_loss / (batch_i + 1)\n",
    "\n",
    "    write_log('Epoch [{}/{}] Finished, Validation Accuracy: {:.2f}%\\n'\n",
    "        .format(epoch+1, num_epochs, val_accuracy), logfile)\n",
    "    end = time.time()\n",
    "    write_log(f\"Time elapsed: {end-start:.02f} seconds\\n\", logfile)\n",
    "    write_log(\"----------------------------------------\\n\", logfile)\n",
    "\n",
    "torch.save(model.state_dict(), join(DATA_DIR, f\"{model_name}_{runstart}_final.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96475d-acfb-4dd9-945b-2be655e455fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icml_1",
   "language": "python",
   "name": "icml_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
